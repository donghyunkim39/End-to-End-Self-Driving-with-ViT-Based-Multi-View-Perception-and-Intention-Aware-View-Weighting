# End-to-End-Self-Driving-with-ViT-Based-Multi-View-Perception-and-Intention-Aware-View-Weighting
A multi-view imageâ€“based lateral control system for autonomous vehicles using conditional imitation learning with left, front, and right camera inputs.

---


ðŸ“Œ Overview
---
This research accompanies the research titled :

"End-to-End Self-Driving with ViT-Based Multi-View Perception and Intention-Aware View Weighting" by Dong-Hyun Kim and Yong-Gu Lee

And

This repository provides a dataset collected in the CARLA simulator (v0.9.15) for End-to-End autonomous driving research. The dataset includes synchronized multi-view camera images (left, front, right), corresponding steering commands, vehicle speed, and High-Level Commands (HLC), all obtained using CARLAâ€™s built-in Autopilot.

ðŸš˜ Research Objective
---
This work proposes an end-to-end autonomous driving model that dynamically adjusts attention across left, front, and right views based on the human driverâ€™s driving intention, and further refines view importance according to road complexity even under the same intention.


---


ðŸŽ¥ Driving Demo
---
### Change to left lane
[![Lane change demo](assets/Change_to_left_lane.png)](assets/Change_to_left_lane.gif)
